{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music Generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dense,Embedding,LSTM,TimeDistributed,Lambda,Dropout,Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/input.txt','r') as file:\n",
    "    data = file.read()\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', '#', '%', '&', \"'\", '(', ')', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', '=', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', '[', '\\\\', ']', '^', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|', '~'] 86\n"
     ]
    }
   ],
   "source": [
    "unique_chars = sorted(set(data))\n",
    "Vocab_length = len(unique_chars)\n",
    "print(unique_chars,Vocab_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_int = dict((c, i) for i, c in enumerate(unique_chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(unique_chars))\n",
    "char_to_int;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[51, 25, 1, 16, 0, 47, 25, 28, 1, 58, 71, 61, 1, 31, 0, 5, 1, 41, 72, 77, 77, 66, 71, 64, 65, 58, 70, 1, 40, 78, 76, 66, 60, 1, 31, 58, 77, 58, 59, 58, 76, 62, 0, 46, 25, 32, 33, 0, 52, 25, 28, 29, 0, 40, 25, 19, 14, 19, 0, 38, 25, 28, 0, 40, 25, 21, 14, 23, 0, 43, 25, 28, 0, 63, 84, 3, 28, 3, 62, 60, 60, 1, 60, 17, 63, 84, 3, 28, 3, 62, 60, 60, 1, 60, 17, 63, 84, 3, 28, 3]\n"
     ]
    }
   ],
   "source": [
    "encoded_text = [char_to_int[value] for value in data]\n",
    "print(encoded_text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "\n",
    "# Tried this way but the present way is more easy \n",
    "\n",
    "# SPLIT_FRAC = 0.751\n",
    "# SPLIT_IDX = int(len(encoded_text)*SPLIT_FRAC)\n",
    "# train_encoded_text = encoded_text[:SPLIT_IDX]\n",
    "# test_encoded_text = encoded_text[SPLIT_IDX:]\n",
    "# len(train_encoded_text),len(test_encoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Preparind batches for train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating  101  batches of size  20  x  64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:00<00:00, 553.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created X_values of (101, 20, 64)  and y_values of (101, 20, 64, 86)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 20\n",
    "SEQUENCE_LENGTH = 64\n",
    "# STEP_LENGTH = 1\n",
    "data_text_length = len(encoded_text)\n",
    "N_BATCHES = (data_text_length // (BATCH_SIZE * SEQUENCE_LENGTH))\n",
    "print('Creating ',N_BATCHES,' batches of size ',BATCH_SIZE,' x ',SEQUENCE_LENGTH)\n",
    "\n",
    "import time\n",
    "time.sleep(0.3)\n",
    "\n",
    "Y_values_cat = to_categorical(encoded_text)\n",
    "X_values= np.zeros([N_BATCHES,BATCH_SIZE,SEQUENCE_LENGTH],dtype=np.int32) #(127, 16, 64)\n",
    "Y_values= np.zeros([N_BATCHES,BATCH_SIZE,SEQUENCE_LENGTH,Vocab_length]) #(127, 16, 64, 86)\n",
    "\n",
    "# temp_data = list(range(data_text_length))\n",
    "for n_b in tqdm(range(N_BATCHES)):\n",
    "    for n_r in range(BATCH_SIZE):\n",
    "        for n_e in range(SEQUENCE_LENGTH):\n",
    "            idx = (n_r*SEQUENCE_LENGTH*N_BATCHES)+(n_b*SEQUENCE_LENGTH)+n_e\n",
    "            X_values[n_b,n_r,n_e] = encoded_text[idx]\n",
    "            Y_values[n_b,n_r,n_e] = Y_values_cat[idx+1]\n",
    "            \n",
    "print('Created X_values of',X_values.shape,' and y_values of',Y_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((101, 15, 64), (101, 15, 64, 86), (101, 5, 64), (101, 5, 64, 86))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train Test Split\n",
    "SPLIT_FRAC = 0.75\n",
    "N_BATCHES_train = int(BATCH_SIZE*SPLIT_FRAC)\n",
    "N_BATCHES_train\n",
    "X_train,X_test = X_values[:,:N_BATCHES_train].copy(),X_values[:,N_BATCHES_train:].copy()\n",
    "Y_train,Y_test = Y_values[:,:N_BATCHES_train].copy(),Y_values[:,N_BATCHES_train:].copy()\n",
    "X_train.shape,Y_train.shape,X_test.shape,Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (20, 64, 512)             44032     \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (20, 64, 256)             787456    \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (20, 64, 256)             0         \n",
      "_________________________________________________________________\n",
      "lstm_13 (LSTM)               (20, 64, 256)             525312    \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (20, 64, 256)             0         \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               (20, 64, 256)             525312    \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (20, 64, 256)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (20, 64, 86)              22102     \n",
      "=================================================================\n",
      "Total params: 1,904,214\n",
      "Trainable params: 1,904,214\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "model.add(Embedding(Vocab_length,512,batch_input_shape=(BATCH_SIZE,SEQUENCE_LENGTH)))\n",
    "model.add(LSTM(256,return_sequences=True,stateful=True))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(256,return_sequences=True,stateful=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256,return_sequences=True,stateful=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(TimeDistributed(Dense(Vocab_length, activation='softmax')))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/500 --> loss = 3.203498431951693, acc = 0.1701423270434215\n",
      "\n",
      "Epoch 2/500 --> loss = 2.345383389161365, acc = 0.33719059411842045\n",
      "\n",
      "Epoch 3/500 --> loss = 1.902235987162826, acc = 0.4428527246017267\n",
      "\n",
      "Epoch 4/500 --> loss = 1.7186124348404384, acc = 0.4959158445938979\n",
      "\n",
      "Epoch 5/500 --> loss = 1.5564737874682586, acc = 0.5299195552816486\n",
      "\n",
      "Epoch 6/500 --> loss = 1.462399930056959, acc = 0.5458694303979968\n",
      "\n",
      "Epoch 7/500 --> loss = 1.393007481452262, acc = 0.5599164641139531\n",
      "\n",
      "Epoch 8/500 --> loss = 1.3430438997721907, acc = 0.5707920766112828\n",
      "\n",
      "Epoch 9/500 --> loss = 1.2990325842753496, acc = 0.5823251852894774\n",
      "\n",
      "Epoch 10/500 --> loss = 1.257094698377175, acc = 0.5950262971443705\n",
      "\n",
      "Epoch 11/500 --> loss = 1.2184118156385895, acc = 0.6066367596682936\n",
      "\n",
      "Epoch 12/500 --> loss = 1.1834336684481932, acc = 0.6153697418694449\n",
      "\n",
      "Epoch 13/500 --> loss = 1.1489565006577143, acc = 0.625990100426249\n",
      "\n",
      "Epoch 14/500 --> loss = 1.1166290215926595, acc = 0.6375618807160028\n",
      "\n",
      "Epoch 15/500 --> loss = 1.0879784090684192, acc = 0.6456992555372786\n",
      "\n",
      "Epoch 16/500 --> loss = 1.059605052565584, acc = 0.6546720301750864\n",
      "\n",
      "Epoch 17/500 --> loss = 1.0321743866004567, acc = 0.6635055665922637\n",
      "\n",
      "Epoch 18/500 --> loss = 1.0078858821698935, acc = 0.6714959805554682\n",
      "\n",
      "Epoch 19/500 --> loss = 0.9829047416696454, acc = 0.6788443710544322\n",
      "\n",
      "Epoch 20/500 --> loss = 0.9607987533701529, acc = 0.687291150045867\n",
      "\n",
      "Epoch 21/500 --> loss = 0.9387523437490558, acc = 0.6928991356698593\n",
      "\n",
      "Epoch 22/500 --> loss = 0.917383354489166, acc = 0.700363550445821\n",
      "\n",
      "Epoch 23/500 --> loss = 0.8942040323030831, acc = 0.7075185640023487\n",
      "\n",
      "Epoch 24/500 --> loss = 0.8735115321555941, acc = 0.7145343416988259\n",
      "\n",
      "Epoch 25/500 --> loss = 0.8517812997987955, acc = 0.7213490103731061\n",
      "\n",
      "Epoch 26/500 --> loss = 0.831001078728402, acc = 0.7278465333551464\n",
      "\n",
      "Epoch 27/500 --> loss = 0.8168202744852199, acc = 0.7316058189562051\n",
      "\n",
      "Epoch 28/500 --> loss = 0.7955190031835349, acc = 0.7378558174218282\n",
      "\n",
      "Epoch 29/500 --> loss = 0.7792123768589284, acc = 0.7445621915382914\n",
      "\n",
      "Epoch 30/500 --> loss = 0.7654747555751612, acc = 0.7482363853124109\n",
      "\n",
      "Epoch 31/500 --> loss = 0.7474432251240948, acc = 0.753387995285563\n",
      "\n",
      "Epoch 32/500 --> loss = 0.7306759085985693, acc = 0.7592125576321441\n",
      "\n",
      "Epoch 33/500 --> loss = 0.7173155617005754, acc = 0.7634204862141373\n",
      "\n",
      "Epoch 34/500 --> loss = 0.7035616978560344, acc = 0.7685024738311768\n",
      "\n",
      "Epoch 35/500 --> loss = 0.6872500594299619, acc = 0.7736927613173381\n",
      "\n",
      "Epoch 36/500 --> loss = 0.6728684518596914, acc = 0.7773824257425742\n",
      "\n",
      "Epoch 37/500 --> loss = 0.6567848230352497, acc = 0.7835860128449922\n",
      "\n",
      "Epoch 38/500 --> loss = 0.6453639321988172, acc = 0.786618193187336\n",
      "\n",
      "Epoch 39/500 --> loss = 0.6279504582433417, acc = 0.7926284009867376\n",
      "\n",
      "Epoch 40/500 --> loss = 0.6179320222080344, acc = 0.7956528468887405\n",
      "\n",
      "Epoch 41/500 --> loss = 0.6055674042442057, acc = 0.7996441822240848\n",
      "\n",
      "Epoch 42/500 --> loss = 0.5946273564702214, acc = 0.8032023511310615\n",
      "\n",
      "Epoch 43/500 --> loss = 0.5800713504304981, acc = 0.8076732695692836\n",
      "\n",
      "Epoch 44/500 --> loss = 0.5688977421510337, acc = 0.810256806930693\n",
      "\n",
      "Epoch 45/500 --> loss = 0.5586683809167088, acc = 0.8138845917021874\n",
      "\n",
      "Epoch 46/500 --> loss = 0.5489469205979074, acc = 0.8175278459445084\n",
      "\n",
      "Epoch 47/500 --> loss = 0.5401617439076452, acc = 0.8210550741393967\n",
      "\n",
      "Epoch 48/500 --> loss = 0.5231798300058534, acc = 0.8267790828600968\n",
      "\n",
      "Epoch 49/500 --> loss = 0.5154781828422358, acc = 0.8276299509671655\n",
      "\n",
      "Epoch 50/500 --> loss = 0.5052001051383443, acc = 0.832031250590145\n",
      "\n",
      "Epoch 51/500 --> loss = 0.4965480383079831, acc = 0.8347230777882113\n",
      "\n",
      "Epoch 52/500 --> loss = 0.4839275249160162, acc = 0.8392094704184202\n",
      "\n",
      "Epoch 53/500 --> loss = 0.47653960237408627, acc = 0.8410659048816945\n",
      "\n",
      "Epoch 54/500 --> loss = 0.4689123901990381, acc = 0.8424272897219894\n",
      "\n",
      "Epoch 55/500 --> loss = 0.4565416645295549, acc = 0.8471612021474555\n",
      "\n",
      "Epoch 56/500 --> loss = 0.44617453719129657, acc = 0.8507967184085657\n",
      "\n",
      "Epoch 57/500 --> loss = 0.44066327220142476, acc = 0.8531714111271471\n",
      "\n",
      "Epoch 58/500 --> loss = 0.4336856824926811, acc = 0.8549350264048813\n",
      "\n",
      "Epoch 59/500 --> loss = 0.42508394293265767, acc = 0.857031247993507\n",
      "\n",
      "Epoch 60/500 --> loss = 0.4211744766424198, acc = 0.8592357688611096\n",
      "\n",
      "Epoch 61/500 --> loss = 0.4108555544130873, acc = 0.8630105180315452\n",
      "\n",
      "Epoch 62/500 --> loss = 0.40034123576513614, acc = 0.8657023523113515\n",
      "\n",
      "Epoch 63/500 --> loss = 0.398710301016817, acc = 0.866197397213171\n",
      "\n",
      "Epoch 64/500 --> loss = 0.3899961707025471, acc = 0.8679764872730369\n",
      "\n",
      "Epoch 65/500 --> loss = 0.3832387254379763, acc = 0.8716738873189038\n",
      "\n",
      "Epoch 66/500 --> loss = 0.3753278352836571, acc = 0.8747447386826619\n",
      "\n",
      "Epoch 67/500 --> loss = 0.3715729542297892, acc = 0.8759050133204697\n",
      "\n",
      "Epoch 68/500 --> loss = 0.3681091143943296, acc = 0.8760287726279532\n",
      "\n",
      "Epoch 69/500 --> loss = 0.3612070272464563, acc = 0.8784576129205156\n",
      "\n",
      "Epoch 70/500 --> loss = 0.35630473258471723, acc = 0.8791460388957864\n",
      "\n",
      "Epoch 71/500 --> loss = 0.3532478384452291, acc = 0.8820003099960856\n",
      "\n",
      "Epoch 72/500 --> loss = 0.3427903168272264, acc = 0.8847076107959936\n",
      "\n",
      "Epoch 73/500 --> loss = 0.338930748181768, acc = 0.8863629350567809\n",
      "\n",
      "Epoch 74/500 --> loss = 0.33513176795279626, acc = 0.8866336640745106\n",
      "\n",
      "Epoch 75/500 --> loss = 0.33342873578024385, acc = 0.8881729557962701\n",
      "\n",
      "Epoch 76/500 --> loss = 0.3309958845672041, acc = 0.8877165830961549\n",
      "\n",
      "Epoch 77/500 --> loss = 0.32439740341488676, acc = 0.8912824899843423\n",
      "\n",
      "Epoch 78/500 --> loss = 0.3216649940108309, acc = 0.8909266697298183\n",
      "\n",
      "Epoch 79/500 --> loss = 0.3134910980663677, acc = 0.8945389820797609\n",
      "\n",
      "Epoch 80/500 --> loss = 0.31152878686933233, acc = 0.894477104786599\n",
      "\n",
      "Epoch 81/500 --> loss = 0.30823620445657485, acc = 0.8962793934463275\n",
      "\n",
      "Epoch 82/500 --> loss = 0.305608743193126, acc = 0.8971689372959704\n",
      "\n",
      "Epoch 83/500 --> loss = 0.2973996920160728, acc = 0.8996596531112595\n",
      "\n",
      "Epoch 84/500 --> loss = 0.29654038645843467, acc = 0.900154702144094\n",
      "\n",
      "Epoch 85/500 --> loss = 0.2914841880302618, acc = 0.9018719031078981\n",
      "\n",
      "Epoch 86/500 --> loss = 0.2891397125060015, acc = 0.9032642327912963\n",
      "\n",
      "Epoch 87/500 --> loss = 0.2858428322147615, acc = 0.9032797022621231\n",
      "\n",
      "Epoch 88/500 --> loss = 0.28235488908715767, acc = 0.9051206672545706\n",
      "\n",
      "Epoch 89/500 --> loss = 0.27689476768569193, acc = 0.9059173866073684\n",
      "\n",
      "Epoch 90/500 --> loss = 0.27586481801354057, acc = 0.9065052607271931\n",
      "\n",
      "Epoch 91/500 --> loss = 0.27117382963695147, acc = 0.9084158439447384\n",
      "\n",
      "Epoch 92/500 --> loss = 0.2704503403441741, acc = 0.9086324271589222\n",
      "\n",
      "Epoch 93/500 --> loss = 0.2694531308837456, acc = 0.9086169553275155\n",
      "\n",
      "Epoch 94/500 --> loss = 0.26395734539716553, acc = 0.9107595930005065\n",
      "\n",
      "Epoch 95/500 --> loss = 0.26300028940238573, acc = 0.9112391737428042\n",
      "\n",
      "Epoch 96/500 --> loss = 0.2561424334745596, acc = 0.9133895405448309\n",
      "\n",
      "Epoch 97/500 --> loss = 0.25582226756775733, acc = 0.9136215985411464\n",
      "\n",
      "Epoch 98/500 --> loss = 0.2541104589063342, acc = 0.9147586645466266\n",
      "\n",
      "Epoch 99/500 --> loss = 0.2506587316494177, acc = 0.915362004596408\n",
      "\n",
      "Epoch 100/500 --> loss = 0.24927975016065163, acc = 0.9153233306242687\n",
      "\n",
      "Epoch 101/500 --> loss = 0.2514425839232926, acc = 0.9152923893220354\n",
      "\n",
      "Epoch 102/500 --> loss = 0.24541300889289025, acc = 0.9168626258868983\n",
      "\n",
      "Epoch 103/500 --> loss = 0.24397519186581715, acc = 0.91724164769201\n",
      "\n",
      "Epoch 104/500 --> loss = 0.24298930802557728, acc = 0.9176129332863459\n",
      "\n",
      "Epoch 105/500 --> loss = 0.24230086493610156, acc = 0.9178681963741189\n",
      "\n",
      "Epoch 106/500 --> loss = 0.23600859467936033, acc = 0.9203279720674648\n",
      "\n",
      "Epoch 107/500 --> loss = 0.233858385446048, acc = 0.9214650368926549\n",
      "\n",
      "Epoch 108/500 --> loss = 0.2319525391748636, acc = 0.9216274754835827\n",
      "\n",
      "Epoch 109/500 --> loss = 0.2312376856213749, acc = 0.9213180730838587\n",
      "\n",
      "Epoch 110/500 --> loss = 0.2293274966206881, acc = 0.922756806458577\n",
      "\n",
      "Epoch 111/500 --> loss = 0.2298184743907192, acc = 0.9217899152548006\n",
      "\n",
      "Epoch 112/500 --> loss = 0.2252914142785686, acc = 0.9239712239492057\n",
      "\n",
      "Epoch 113/500 --> loss = 0.22730867033547694, acc = 0.9239325505672115\n",
      "\n",
      "Epoch 114/500 --> loss = 0.2231123820684924, acc = 0.9245358906169929\n",
      "\n",
      "Epoch 115/500 --> loss = 0.22005822177570644, acc = 0.9253558168316832\n",
      "\n",
      "Epoch 116/500 --> loss = 0.21948746304110725, acc = 0.9262840322928854\n",
      "\n",
      "Epoch 117/500 --> loss = 0.21733351346879903, acc = 0.92650835230799\n",
      "\n",
      "Epoch 118/500 --> loss = 0.2168825605128071, acc = 0.9269569941086344\n",
      "\n",
      "Epoch 119/500 --> loss = 0.21791076350330127, acc = 0.9259204787783103\n",
      "\n",
      "Epoch 120/500 --> loss = 0.2161669149847314, acc = 0.9267404055831456\n",
      "\n",
      "Epoch 121/500 --> loss = 0.21094472352230903, acc = 0.9291073668121111\n",
      "\n",
      "Epoch 122/500 --> loss = 0.2134269486264427, acc = 0.927869740689155\n",
      "\n",
      "Epoch 123/500 --> loss = 0.21589630473368238, acc = 0.9265857043832836\n",
      "\n",
      "Epoch 124/500 --> loss = 0.20919185286701317, acc = 0.9295095919382454\n",
      "\n",
      "Epoch 125/500 --> loss = 0.208062345733737, acc = 0.9295405314700438\n",
      "\n",
      "Epoch 126/500 --> loss = 0.20655875852202424, acc = 0.9302366948363805\n",
      "\n",
      "Epoch 127/500 --> loss = 0.206851591568182, acc = 0.9297261751524293\n",
      "\n",
      "Epoch 128/500 --> loss = 0.20521035052762174, acc = 0.9311494396464659\n",
      "\n",
      "Epoch 129/500 --> loss = 0.20512046613315543, acc = 0.9308245660054801\n",
      "\n",
      "Epoch 130/500 --> loss = 0.19953126600473234, acc = 0.9323638600878196\n",
      "\n",
      "Epoch 131/500 --> loss = 0.20384707722333398, acc = 0.9319074873877043\n",
      "\n",
      "Epoch 132/500 --> loss = 0.20115204140691473, acc = 0.9326191196347228\n",
      "\n",
      "Epoch 133/500 --> loss = 0.19900778835952873, acc = 0.9330213488918719\n",
      "\n",
      "Epoch 134/500 --> loss = 0.2043629763740124, acc = 0.9312732078061245\n",
      "\n",
      "Epoch 135/500 --> loss = 0.19637448584089184, acc = 0.933384900045867\n",
      "\n",
      "Epoch 136/500 --> loss = 0.19246089251914827, acc = 0.9352568063405481\n",
      "\n",
      "Epoch 137/500 --> loss = 0.19572197520496823, acc = 0.9337097742769978\n",
      "\n",
      "Epoch 138/500 --> loss = 0.19548155898504918, acc = 0.933384901226157\n",
      "\n",
      "Epoch 139/500 --> loss = 0.18932580372484603, acc = 0.9364248124679716\n",
      "\n",
      "Epoch 140/500 --> loss = 0.1921142103648422, acc = 0.9352800108418606\n",
      "\n",
      "Epoch 141/500 --> loss = 0.18723792929460506, acc = 0.9372756793947503\n",
      "\n",
      "Epoch 142/500 --> loss = 0.19034171856866025, acc = 0.9351794554455446\n",
      "\n",
      "Epoch 143/500 --> loss = 0.18863402764395912, acc = 0.9367264851485149\n",
      "\n",
      "Epoch 144/500 --> loss = 0.18666471954029384, acc = 0.9371055069536266\n",
      "\n",
      "Epoch 145/500 --> loss = 0.18492101073855222, acc = 0.9376933810734512\n",
      "\n",
      "Epoch 146/500 --> loss = 0.18529597559187672, acc = 0.9372988874369329\n",
      "\n",
      "Epoch 147/500 --> loss = 0.18679530416974927, acc = 0.9360225861615473\n",
      "\n",
      "Epoch 148/500 --> loss = 0.1842889503972365, acc = 0.9383586019572645\n",
      "\n",
      "Epoch 149/500 --> loss = 0.18325348227921098, acc = 0.9378016709101082\n",
      "\n",
      "Epoch 150/500 --> loss = 0.18415515759203693, acc = 0.9372679467248445\n",
      "\n",
      "Epoch 151/500 --> loss = 0.18341410204325573, acc = 0.937237007193046\n",
      "\n",
      "Epoch 152/500 --> loss = 0.1809997048118327, acc = 0.9387917695659223\n",
      "\n",
      "Epoch 153/500 --> loss = 0.18299465383043384, acc = 0.9388304435380614\n",
      "\n",
      "Epoch 154/500 --> loss = 0.18140145162544627, acc = 0.9394337865385679\n",
      "\n",
      "Epoch 155/500 --> loss = 0.1798687386925858, acc = 0.9392017314929774\n",
      "\n",
      "Epoch 156/500 --> loss = 0.1775004909475251, acc = 0.9403852106320976\n",
      "\n",
      "Epoch 157/500 --> loss = 0.1774412995517844, acc = 0.9403774738311768\n",
      "\n",
      "Epoch 158/500 --> loss = 0.17602010986002364, acc = 0.9403387986787475\n",
      "\n",
      "Epoch 159/500 --> loss = 0.17697628550600297, acc = 0.9400758052816486\n",
      "\n",
      "Epoch 160/500 --> loss = 0.17528739173223476, acc = 0.9403774708804518\n",
      "\n",
      "Epoch 161/500 --> loss = 0.17256468143498543, acc = 0.9411741949544095\n",
      "\n",
      "Epoch 162/500 --> loss = 0.17136188884182732, acc = 0.9421797636711952\n",
      "\n",
      "Epoch 163/500 --> loss = 0.17166931219030135, acc = 0.9422957938496429\n",
      "\n",
      "Epoch 164/500 --> loss = 0.169293472672453, acc = 0.9426129342305778\n",
      "\n",
      "Epoch 165/500 --> loss = 0.16700145683371195, acc = 0.9434483293259498\n",
      "\n",
      "Epoch 166/500 --> loss = 0.17344793175706769, acc = 0.9414681331946118\n",
      "\n",
      "Epoch 167/500 --> loss = 0.17145472794476121, acc = 0.94135983568607\n",
      "\n",
      "Epoch 168/500 --> loss = 0.17264192721041122, acc = 0.9415996310734512\n",
      "\n",
      "Epoch 169/500 --> loss = 0.17163532662509692, acc = 0.9416305711953947\n",
      "\n",
      "Epoch 170/500 --> loss = 0.17564637088539578, acc = 0.9414526601829151\n",
      "\n",
      "Epoch 171/500 --> loss = 0.17031661723509872, acc = 0.9425278477149435\n",
      "\n",
      "Epoch 172/500 --> loss = 0.1646840373183241, acc = 0.9445235139072532\n",
      "\n",
      "Epoch 173/500 --> loss = 0.16656831600288352, acc = 0.9443765447871519\n",
      "\n",
      "Epoch 174/500 --> loss = 0.16552899571338503, acc = 0.9448329198478472\n",
      "\n",
      "Epoch 175/500 --> loss = 0.16828120005602884, acc = 0.9426206716216436\n",
      "\n",
      "Epoch 176/500 --> loss = 0.16706932181178932, acc = 0.9442837250114667\n",
      "\n",
      "Epoch 177/500 --> loss = 0.16606589532134555, acc = 0.9440748744671887\n",
      "\n",
      "Epoch 178/500 --> loss = 0.16302055725366763, acc = 0.944948947665715\n",
      "\n",
      "Epoch 179/500 --> loss = 0.1623675545843521, acc = 0.9448019814963388\n",
      "\n",
      "Epoch 180/500 --> loss = 0.16513332562281355, acc = 0.9446704830273543\n",
      "\n",
      "Epoch 181/500 --> loss = 0.16534263825062478, acc = 0.9442450498590375\n",
      "\n",
      "Epoch 182/500 --> loss = 0.1620926598068511, acc = 0.9449025380729449\n",
      "\n",
      "Epoch 183/500 --> loss = 0.16099092836427217, acc = 0.945730199908266\n",
      "\n",
      "Epoch 184/500 --> loss = 0.1612413822129221, acc = 0.9463876851714483\n",
      "\n",
      "Epoch 185/500 --> loss = 0.16162304735124702, acc = 0.9450959156055262\n",
      "\n",
      "Epoch 186/500 --> loss = 0.1616755574941635, acc = 0.9454439984689845\n",
      "\n",
      "Epoch 187/500 --> loss = 0.1616013283924301, acc = 0.9453743831946118\n",
      "\n",
      "Epoch 188/500 --> loss = 0.16177256501252107, acc = 0.9452351496951414\n",
      "\n",
      "Epoch 189/500 --> loss = 0.15823018130394492, acc = 0.946875000944232\n",
      "\n",
      "Epoch 190/500 --> loss = 0.16124791098703253, acc = 0.9462020438496429\n",
      "\n",
      "Epoch 191/500 --> loss = 0.16096761748932376, acc = 0.9455754963478239\n",
      "\n",
      "Epoch 192/500 --> loss = 0.1573978982644506, acc = 0.9461711007769745\n",
      "\n",
      "Epoch 193/500 --> loss = 0.1566611563067625, acc = 0.9470219682938982\n",
      "\n",
      "Epoch 194/500 --> loss = 0.1561564994182917, acc = 0.9473623161268706\n",
      "\n",
      "Epoch 195/500 --> loss = 0.15514524384300307, acc = 0.9474706041930926\n",
      "\n",
      "Epoch 196/500 --> loss = 0.15631047156777714, acc = 0.9478496289489293\n",
      "\n",
      "Epoch 197/500 --> loss = 0.15369039412477228, acc = 0.9484684390596824\n",
      "\n",
      "Epoch 198/500 --> loss = 0.1588319197739705, acc = 0.9465733312144138\n",
      "\n",
      "Epoch 199/500 --> loss = 0.15715419781385082, acc = 0.9471844051143911\n",
      "\n",
      "Epoch 200/500 --> loss = 0.1548407091067569, acc = 0.9476871938988713\n",
      "\n",
      "Epoch 201/500 --> loss = 0.15440736762662927, acc = 0.9469446168087496\n",
      "\n",
      "Epoch 202/500 --> loss = 0.15402254934358126, acc = 0.9482131812832143\n",
      "\n",
      "Epoch 203/500 --> loss = 0.1511065518797034, acc = 0.9486308782407553\n",
      "\n",
      "Epoch 204/500 --> loss = 0.15375771956278547, acc = 0.9474628662118817\n",
      "\n",
      "Epoch 205/500 --> loss = 0.15155358796957696, acc = 0.9491181928332489\n",
      "\n",
      "Epoch 206/500 --> loss = 0.15338296007992017, acc = 0.9487546387285289\n",
      "\n",
      "Epoch 207/500 --> loss = 0.15294589677659592, acc = 0.9484916482821549\n",
      "\n",
      "Epoch 208/500 --> loss = 0.15449253159879459, acc = 0.948073948373889\n",
      "\n",
      "Epoch 209/500 --> loss = 0.15300515758814198, acc = 0.9483756228248672\n",
      "\n",
      "Epoch 210/500 --> loss = 0.15144146964101507, acc = 0.9485767342076443\n",
      "\n",
      "Epoch 211/500 --> loss = 0.14854387990613976, acc = 0.9494585388957864\n",
      "\n",
      "Epoch 212/500 --> loss = 0.14987175970679462, acc = 0.9495977706248218\n",
      "\n",
      "Epoch 213/500 --> loss = 0.14785925070248027, acc = 0.950572400399954\n",
      "\n",
      "Epoch 214/500 --> loss = 0.14975385463768892, acc = 0.9493502502394194\n",
      "\n",
      "Epoch 215/500 --> loss = 0.15150241819348667, acc = 0.9494585406662214\n",
      "\n",
      "Epoch 216/500 --> loss = 0.14839737331218059, acc = 0.9499690573994476\n",
      "\n",
      "Epoch 217/500 --> loss = 0.15034734295441374, acc = 0.9487082332667738\n",
      "\n",
      "Epoch 218/500 --> loss = 0.1472483836483247, acc = 0.9499922666219202\n",
      "\n",
      "Epoch 219/500 --> loss = 0.14767224614572996, acc = 0.950363551035966\n",
      "\n",
      "Epoch 220/500 --> loss = 0.14799942194235208, acc = 0.9504331674906287\n",
      "\n",
      "Epoch 221/500 --> loss = 0.14442842387326874, acc = 0.9512298856631364\n",
      "\n",
      "Epoch 222/500 --> loss = 0.1466409043215289, acc = 0.9505956084421365\n",
      "\n",
      "Epoch 223/500 --> loss = 0.14633237150045905, acc = 0.9504331680807737\n",
      "\n",
      "Epoch 224/500 --> loss = 0.14744996913883945, acc = 0.9492419556816025\n",
      "\n",
      "Epoch 225/500 --> loss = 0.14561846653128616, acc = 0.9510133048095325\n",
      "\n",
      "Epoch 226/500 --> loss = 0.14648038156256818, acc = 0.9501701729132397\n",
      "\n",
      "Epoch 227/500 --> loss = 0.1446154682648064, acc = 0.9512608316865298\n",
      "\n",
      "Epoch 228/500 --> loss = 0.1490022425751875, acc = 0.949582298793415\n",
      "\n",
      "Epoch 229/500 --> loss = 0.14522156023448057, acc = 0.9515006158611562\n",
      "\n",
      "Epoch 230/500 --> loss = 0.14448116631200997, acc = 0.9511138619762836\n",
      "\n",
      "Epoch 231/500 --> loss = 0.14667759179183754, acc = 0.9506110779129633\n",
      "\n",
      "Epoch 232/500 --> loss = 0.14626113981893746, acc = 0.9505569297488373\n",
      "\n",
      "Epoch 233/500 --> loss = 0.1459099147284385, acc = 0.9504641093830071\n",
      "\n",
      "Epoch 234/500 --> loss = 0.14137744874057204, acc = 0.9524443049242001\n",
      "\n",
      "Epoch 235/500 --> loss = 0.14610287702024574, acc = 0.9511370658874512\n",
      "\n",
      "Epoch 236/500 --> loss = 0.1417252655961726, acc = 0.9521116933020035\n",
      "\n",
      "Epoch 237/500 --> loss = 0.14228437737663194, acc = 0.9517249411875659\n",
      "\n",
      "Epoch 238/500 --> loss = 0.1437129758224629, acc = 0.951353652642505\n",
      "\n",
      "Epoch 239/500 --> loss = 0.14449771287122576, acc = 0.9513072412792999\n",
      "\n",
      "Epoch 240/500 --> loss = 0.1439491016882481, acc = 0.951353649101635\n",
      "\n",
      "Epoch 241/500 --> loss = 0.14294792276502835, acc = 0.9518100265229102\n",
      "\n",
      "Epoch 242/500 --> loss = 0.139066647553798, acc = 0.953542699318121\n",
      "\n",
      "Epoch 243/500 --> loss = 0.14373174169571093, acc = 0.9510519793718168\n",
      "\n",
      "Epoch 244/500 --> loss = 0.1427076163120789, acc = 0.9518564372959704\n",
      "\n",
      "Epoch 245/500 --> loss = 0.14131993984822, acc = 0.9518951106779646\n",
      "\n",
      "Epoch 246/500 --> loss = 0.1414027225853193, acc = 0.9529316195166937\n",
      "\n",
      "Epoch 247/500 --> loss = 0.13844994433445507, acc = 0.953434404760304\n",
      "\n",
      "Epoch 248/500 --> loss = 0.14109858774607725, acc = 0.9520884923415609\n",
      "\n",
      "Epoch 249/500 --> loss = 0.13965932685549898, acc = 0.9524675129663827\n",
      "\n",
      "Epoch 250/500 --> loss = 0.1401811613157244, acc = 0.9527691821060559\n",
      "\n",
      "Epoch 251/500 --> loss = 0.13899820652043465, acc = 0.9529393574979046\n",
      "\n",
      "Epoch 252/500 --> loss = 0.14031254728831868, acc = 0.9524752479968684\n",
      "\n",
      "Epoch 253/500 --> loss = 0.14035204414388922, acc = 0.9521735759064702\n",
      "\n",
      "Epoch 254/500 --> loss = 0.14092326665868854, acc = 0.952498454268616\n",
      "\n",
      "Epoch 255/500 --> loss = 0.14137704382733543, acc = 0.9523514869189499\n",
      "\n",
      "Epoch 256/500 --> loss = 0.13953756659042718, acc = 0.953333848773843\n",
      "\n",
      "Epoch 257/500 --> loss = 0.13832520066511514, acc = 0.9539758651563437\n",
      "\n",
      "Epoch 258/500 --> loss = 0.13746054101698468, acc = 0.9535813715198252\n",
      "\n",
      "Epoch 259/500 --> loss = 0.1366511711684784, acc = 0.95416924091849\n",
      "\n",
      "Epoch 260/500 --> loss = 0.1372892303336965, acc = 0.9534730816831684\n",
      "\n",
      "Epoch 261/500 --> loss = 0.13673775230008778, acc = 0.9536896672579321\n",
      "\n",
      "Epoch 262/500 --> loss = 0.1379396664624167, acc = 0.954215658183145\n",
      "\n",
      "Epoch 263/500 --> loss = 0.13700476480592597, acc = 0.9539139884533269\n",
      "\n",
      "Epoch 264/500 --> loss = 0.1365643902727873, acc = 0.953442142741515\n",
      "\n",
      "Epoch 265/500 --> loss = 0.13945102920331578, acc = 0.9530863213067008\n",
      "\n",
      "Epoch 266/500 --> loss = 0.1354064897145375, acc = 0.9545250635335941\n",
      "\n",
      "Epoch 267/500 --> loss = 0.13512151444902515, acc = 0.9543935656547546\n",
      "\n",
      "Epoch 268/500 --> loss = 0.13451839039231292, acc = 0.9540764252738198\n",
      "\n",
      "Epoch 269/500 --> loss = 0.13359633479082939, acc = 0.9548499395351598\n",
      "\n",
      "Epoch 270/500 --> loss = 0.1383539651437561, acc = 0.9537360738999773\n",
      "\n",
      "Epoch 271/500 --> loss = 0.1379909051379355, acc = 0.9531868796537418\n",
      "\n",
      "Epoch 272/500 --> loss = 0.13554679403210632, acc = 0.9539913358074603\n",
      "\n",
      "Epoch 273/500 --> loss = 0.13784733628577525, acc = 0.953380264858208\n",
      "\n",
      "Epoch 274/500 --> loss = 0.1385476730099999, acc = 0.9530553841354823\n",
      "\n",
      "Epoch 275/500 --> loss = 0.13860131652638463, acc = 0.9532951777524287\n",
      "\n",
      "Epoch 276/500 --> loss = 0.13649246974451706, acc = 0.9535117556553075\n",
      "\n",
      "Epoch 277/500 --> loss = 0.1386013005924697, acc = 0.9530167131140681\n",
      "\n",
      "Epoch 278/500 --> loss = 0.13720007380931684, acc = 0.9538907774604193\n",
      "\n",
      "Epoch 279/500 --> loss = 0.13330878273095234, acc = 0.955105197901773\n",
      "\n",
      "Epoch 280/500 --> loss = 0.13498862397552716, acc = 0.9549272904301634\n",
      "\n",
      "Epoch 281/500 --> loss = 0.13454027658346857, acc = 0.9550123710443478\n",
      "\n",
      "Epoch 282/500 --> loss = 0.1362396803232703, acc = 0.9548808784768132\n",
      "\n",
      "Epoch 283/500 --> loss = 0.13579516562789973, acc = 0.9542775360664518\n",
      "\n",
      "Epoch 284/500 --> loss = 0.1342396044937691, acc = 0.9546256165693302\n",
      "\n",
      "Epoch 285/500 --> loss = 0.1327269078776388, acc = 0.9555847756933458\n",
      "\n",
      "Epoch 286/500 --> loss = 0.13243452230892558, acc = 0.9552444325815334\n",
      "\n",
      "Epoch 287/500 --> loss = 0.13514389621444267, acc = 0.9541383061078516\n",
      "\n",
      "Epoch 288/500 --> loss = 0.13490825847233875, acc = 0.9541305722576556\n",
      "\n",
      "Epoch 289/500 --> loss = 0.13252632917449025, acc = 0.955368190118582\n",
      "\n",
      "Epoch 290/500 --> loss = 0.13589055696041277, acc = 0.954633354550541\n",
      "\n",
      "Epoch 291/500 --> loss = 0.13441921726311787, acc = 0.9545560030653926\n",
      "\n",
      "Epoch 292/500 --> loss = 0.13404369472277047, acc = 0.95525990087207\n",
      "\n",
      "Epoch 293/500 --> loss = 0.13329091424694156, acc = 0.9553991343715403\n",
      "\n",
      "Epoch 294/500 --> loss = 0.13477730020733164, acc = 0.9544322396268939\n",
      "\n",
      "Epoch 295/500 --> loss = 0.1317316413958474, acc = 0.9550587871287128\n",
      "\n",
      "Epoch 296/500 --> loss = 0.13319504888045905, acc = 0.9550897296112363\n",
      "\n",
      "Epoch 297/500 --> loss = 0.13820936294770478, acc = 0.9533029104223346\n",
      "\n",
      "Epoch 298/500 --> loss = 0.13176251195444919, acc = 0.9550123793063777\n",
      "\n",
      "Epoch 299/500 --> loss = 0.13279563218060106, acc = 0.9557936244671887\n",
      "\n",
      "Epoch 300/500 --> loss = 0.13246681172363828, acc = 0.9550974616909972\n",
      "\n",
      "Epoch 301/500 --> loss = 0.12865400705302116, acc = 0.9564743183626987\n",
      "\n",
      "Epoch 302/500 --> loss = 0.13009166267543737, acc = 0.9557704187855862\n",
      "\n",
      "Epoch 303/500 --> loss = 0.1296946064080342, acc = 0.9557085397219894\n",
      "\n",
      "Epoch 304/500 --> loss = 0.13076369257846682, acc = 0.9556311882368409\n",
      "\n",
      "Epoch 305/500 --> loss = 0.1286255260652835, acc = 0.9565516680774122\n",
      "\n",
      "Epoch 306/500 --> loss = 0.12627968841260023, acc = 0.9576036511081281\n",
      "\n",
      "Epoch 307/500 --> loss = 0.12754457967706245, acc = 0.9569074871516464\n",
      "\n",
      "Epoch 308/500 --> loss = 0.12852925246599878, acc = 0.9561881169234172\n",
      "\n",
      "Epoch 309/500 --> loss = 0.1307997126685511, acc = 0.9551670763752248\n",
      "\n",
      "Epoch 310/500 --> loss = 0.13031669602830812, acc = 0.9563041482821549\n",
      "\n",
      "Epoch 311/500 --> loss = 0.12906798417910492, acc = 0.9565207303160488\n",
      "\n",
      "Epoch 312/500 --> loss = 0.13069141479116855, acc = 0.9557240109632511\n",
      "\n",
      "Epoch 313/500 --> loss = 0.1297816438692631, acc = 0.9562809378793924\n",
      "\n",
      "Epoch 314/500 --> loss = 0.13010909260794667, acc = 0.9563815003574485\n",
      "\n",
      "Epoch 315/500 --> loss = 0.1281784448293176, acc = 0.9570621936628134\n",
      "\n",
      "Epoch 316/500 --> loss = 0.1281614607513541, acc = 0.9570080454986875\n",
      "\n",
      "Epoch 317/500 --> loss = 0.130005974064369, acc = 0.9561107677988486\n",
      "\n",
      "Epoch 318/500 --> loss = 0.12816139081917186, acc = 0.9564897901941054\n",
      "\n",
      "Epoch 319/500 --> loss = 0.12931909117073115, acc = 0.9563505555143451\n",
      "\n",
      "Epoch 320/500 --> loss = 0.13113412054458468, acc = 0.9565903473608565\n",
      "\n",
      "Epoch 321/500 --> loss = 0.12803750108964373, acc = 0.9566831683168316\n",
      "\n",
      "Epoch 322/500 --> loss = 0.12748879526216206, acc = 0.9563428228444392\n",
      "\n",
      "Epoch 323/500 --> loss = 0.13282878726425737, acc = 0.9549969068848261\n",
      "\n",
      "Epoch 324/500 --> loss = 0.12720699032934585, acc = 0.9575108289718628\n",
      "\n",
      "Epoch 325/500 --> loss = 0.12972615091222348, acc = 0.9562267944364264\n",
      "\n",
      "Epoch 326/500 --> loss = 0.12805680952744908, acc = 0.9564743171824087\n",
      "\n",
      "Epoch 327/500 --> loss = 0.12896220186854354, acc = 0.9571550134384986\n",
      "\n",
      "Epoch 328/500 --> loss = 0.12877044546427113, acc = 0.95703125236058\n",
      "\n",
      "Epoch 329/500 --> loss = 0.12907814500060413, acc = 0.9566522270145983\n",
      "\n",
      "Epoch 330/500 --> loss = 0.13171657978898227, acc = 0.9560256783324893\n",
      "\n",
      "Epoch 331/500 --> loss = 0.13027887267641503, acc = 0.9559947417514159\n",
      "\n",
      "Epoch 332/500 --> loss = 0.13007790720698856, acc = 0.9560256812832143\n",
      "\n",
      "Epoch 333/500 --> loss = 0.12631865207216528, acc = 0.9572168925020954\n",
      "\n",
      "Epoch 334/500 --> loss = 0.12706930485397283, acc = 0.9567759892728069\n",
      "\n",
      "Epoch 335/500 --> loss = 0.12527161689087896, acc = 0.9574025350041909\n",
      "\n",
      "Epoch 336/500 --> loss = 0.12533511332061031, acc = 0.9575881816373013\n",
      "\n",
      "Epoch 337/500 --> loss = 0.126487700729677, acc = 0.9579981447446464\n",
      "\n",
      "Epoch 338/500 --> loss = 0.12876026384016076, acc = 0.9567914563830536\n",
      "\n",
      "Epoch 339/500 --> loss = 0.12544945723349504, acc = 0.9568456063176146\n",
      "\n",
      "Epoch 340/500 --> loss = 0.12350731667610679, acc = 0.9580987001409625\n",
      "\n",
      "Epoch 341/500 --> loss = 0.12399563112176291, acc = 0.9582688743525213\n",
      "\n",
      "Epoch 342/500 --> loss = 0.12727240340249374, acc = 0.9569152221821322\n",
      "\n",
      "Epoch 343/500 --> loss = 0.13051123614653504, acc = 0.9559096534653465\n",
      "\n",
      "Epoch 344/500 --> loss = 0.12977370037005678, acc = 0.9560643558454985\n",
      "\n",
      "Epoch 345/500 --> loss = 0.12898064404726028, acc = 0.9569229560323281\n",
      "\n",
      "Epoch 346/500 --> loss = 0.12657809810768259, acc = 0.957348392741515\n",
      "\n",
      "Epoch 347/500 --> loss = 0.12890557487412255, acc = 0.9563196183431266\n",
      "\n",
      "Epoch 348/500 --> loss = 0.12450103742061275, acc = 0.9578434376433345\n",
      "\n",
      "Epoch 349/500 --> loss = 0.12617993458072738, acc = 0.957340655940594\n",
      "\n",
      "Epoch 350/500 --> loss = 0.12515984372337266, acc = 0.9578434394137694\n",
      "\n",
      "Epoch 351/500 --> loss = 0.12452076427122154, acc = 0.9576887358533274\n",
      "\n",
      "Epoch 352/500 --> loss = 0.12593607080749947, acc = 0.9579130558684321\n",
      "\n",
      "Epoch 353/500 --> loss = 0.12305256016183608, acc = 0.9582611346008754\n",
      "\n",
      "Epoch 354/500 --> loss = 0.12353173540075227, acc = 0.9582611410924704\n",
      "\n",
      "Epoch 355/500 --> loss = 0.1265180634685082, acc = 0.9577119403546399\n",
      "\n",
      "Epoch 356/500 --> loss = 0.12384777825952757, acc = 0.9579053220182362\n",
      "\n",
      "Epoch 357/500 --> loss = 0.12404939517526344, acc = 0.9577196801062857\n",
      "\n",
      "Epoch 358/500 --> loss = 0.12548981570076234, acc = 0.957657794551094\n",
      "\n",
      "Epoch 359/500 --> loss = 0.12539283016530595, acc = 0.9578124987016811\n",
      "\n",
      "Epoch 360/500 --> loss = 0.12549091049350133, acc = 0.9577274127761917\n",
      "\n",
      "Epoch 361/500 --> loss = 0.12018802452205431, acc = 0.9592125669564351\n",
      "\n",
      "Epoch 362/500 --> loss = 0.1240413158689395, acc = 0.9576268585601656\n",
      "\n",
      "Epoch 363/500 --> loss = 0.12665065547617355, acc = 0.9570235137892241\n",
      "\n",
      "Epoch 364/500 --> loss = 0.12519223958548933, acc = 0.9578898501868295\n",
      "\n",
      "Epoch 365/500 --> loss = 0.12337863246108045, acc = 0.9580986948296575\n",
      "\n",
      "Epoch 366/500 --> loss = 0.1229780909448567, acc = 0.9585009299882568\n",
      "\n",
      "Epoch 367/500 --> loss = 0.12375685380827083, acc = 0.9579517339715863\n",
      "\n",
      "Epoch 368/500 --> loss = 0.12685465473349733, acc = 0.957394801153995\n",
      "\n",
      "Epoch 369/500 --> loss = 0.12371660518174124, acc = 0.9578279681725077\n",
      "\n",
      "Epoch 370/500 --> loss = 0.12490730415476431, acc = 0.9578821157464886\n",
      "\n",
      "Epoch 371/500 --> loss = 0.12043246464564068, acc = 0.958957302688372\n",
      "\n",
      "Epoch 372/500 --> loss = 0.12087731410076122, acc = 0.958647893797053\n",
      "\n",
      "Epoch 373/500 --> loss = 0.12345865272944516, acc = 0.9587561889450149\n",
      "\n",
      "Epoch 374/500 --> loss = 0.121693720838221, acc = 0.9586324255065163\n",
      "\n",
      "Epoch 375/500 --> loss = 0.12050907262186013, acc = 0.9596070588225185\n",
      "\n",
      "Epoch 376/500 --> loss = 0.12119976018029864, acc = 0.9588258048095325\n",
      "\n",
      "Epoch 377/500 --> loss = 0.12385018110865413, acc = 0.9582069946987795\n",
      "\n",
      "Epoch 378/500 --> loss = 0.12373904500267294, acc = 0.9577738264999768\n",
      "\n",
      "Epoch 379/500 --> loss = 0.12481026550625811, acc = 0.9574412148777801\n",
      "\n",
      "Epoch 380/500 --> loss = 0.12219007490294995, acc = 0.9586014871550078\n",
      "\n",
      "Epoch 381/500 --> loss = 0.12190054489834475, acc = 0.9584699869155884\n",
      "\n",
      "Epoch 382/500 --> loss = 0.123543416583302, acc = 0.9582224641696061\n",
      "\n",
      "Epoch 383/500 --> loss = 0.12216604744443799, acc = 0.9587097775818098\n",
      "\n",
      "Epoch 384/500 --> loss = 0.12285992532673448, acc = 0.9584081078519916\n",
      "\n",
      "Epoch 385/500 --> loss = 0.12370974091019961, acc = 0.9581915216870828\n",
      "\n",
      "Epoch 386/500 --> loss = 0.12485423831656428, acc = 0.9575881810471563\n",
      "\n",
      "Epoch 387/500 --> loss = 0.12271066614896944, acc = 0.9586788350992864\n",
      "\n",
      "Epoch 388/500 --> loss = 0.12291487269472368, acc = 0.9585860170940361\n",
      "\n",
      "Epoch 389/500 --> loss = 0.1195958852030263, acc = 0.9597153498394655\n",
      "\n",
      "Epoch 390/500 --> loss = 0.12295641897633525, acc = 0.958439047973935\n",
      "\n",
      "Epoch 391/500 --> loss = 0.12380085206858002, acc = 0.9582147261883953\n",
      "\n",
      "Epoch 392/500 --> loss = 0.12021962177045274, acc = 0.95916615087207\n",
      "\n",
      "Epoch 393/500 --> loss = 0.12197715973499978, acc = 0.9587871284768132\n",
      "\n",
      "Epoch 394/500 --> loss = 0.12423569413990078, acc = 0.9582147279588302\n",
      "\n",
      "Epoch 395/500 --> loss = 0.12367886290101722, acc = 0.9586169578061245\n",
      "\n",
      "Epoch 396/500 --> loss = 0.1255117472593147, acc = 0.9577119462560899\n",
      "\n",
      "Epoch 397/500 --> loss = 0.1253792956323907, acc = 0.957603650517983\n",
      "\n",
      "Epoch 398/500 --> loss = 0.12158960155626335, acc = 0.9584235785031082\n",
      "\n",
      "Epoch 399/500 --> loss = 0.11989557034898513, acc = 0.9593595284046513\n",
      "\n",
      "Epoch 400/500 --> loss = 0.12251012160046267, acc = 0.9587793904956025\n",
      "\n",
      "Epoch 401/500 --> loss = 0.12093194640508972, acc = 0.9594136742081972\n",
      "\n",
      "Epoch 402/500 --> loss = 0.11910123665734093, acc = 0.9596844038160721\n",
      "\n",
      "Epoch 403/500 --> loss = 0.11820002460833823, acc = 0.9600789027638955\n",
      "\n",
      "Epoch 404/500 --> loss = 0.12062936113907559, acc = 0.9590269179627446\n",
      "\n",
      "Epoch 405/500 --> loss = 0.12096198603953465, acc = 0.9590501242344922\n",
      "\n",
      "Epoch 406/500 --> loss = 0.1217747816059849, acc = 0.9586401617172922\n",
      "\n",
      "Epoch 407/500 --> loss = 0.12011336387679128, acc = 0.9595219652251442\n",
      "\n",
      "Epoch 408/500 --> loss = 0.12032974872848776, acc = 0.9599628719953027\n",
      "\n",
      "Epoch 409/500 --> loss = 0.12237407348238595, acc = 0.9587561883548699\n",
      "\n",
      "Epoch 410/500 --> loss = 0.12052425424946417, acc = 0.9590965320568273\n",
      "\n",
      "Epoch 411/500 --> loss = 0.12161961507679213, acc = 0.959166150281925\n",
      "\n",
      "Epoch 412/500 --> loss = 0.12123575120574177, acc = 0.959065592525029\n",
      "\n",
      "Epoch 413/500 --> loss = 0.11932472006814314, acc = 0.9598545797980658\n",
      "\n",
      "Epoch 414/500 --> loss = 0.1205784605428724, acc = 0.9587097775818098\n",
      "\n",
      "Epoch 415/500 --> loss = 0.12073351752639998, acc = 0.9595915828600968\n",
      "\n",
      "Epoch 416/500 --> loss = 0.12051782137391591, acc = 0.9589882463511854\n",
      "\n",
      "Epoch 417/500 --> loss = 0.11848800688391865, acc = 0.9602181297717708\n",
      "\n",
      "Epoch 418/500 --> loss = 0.1213810602479642, acc = 0.9591274774900758\n",
      "\n",
      "Epoch 419/500 --> loss = 0.12000643970942733, acc = 0.9597153468887405\n",
      "\n",
      "Epoch 420/500 --> loss = 0.1198229462203413, acc = 0.9594523511310615\n",
      "\n",
      "Epoch 421/500 --> loss = 0.11938250566473102, acc = 0.9592899154908586\n",
      "\n",
      "Epoch 422/500 --> loss = 0.12227126565014962, acc = 0.9592357661464427\n",
      "\n",
      "Epoch 423/500 --> loss = 0.11971334146686119, acc = 0.959583848419756\n",
      "\n",
      "Epoch 424/500 --> loss = 0.12302337181155044, acc = 0.9581373770638267\n",
      "\n",
      "Epoch 425/500 --> loss = 0.12002887798122841, acc = 0.9594446149202857\n",
      "\n",
      "Epoch 426/500 --> loss = 0.12152715669114991, acc = 0.9592976499311995\n",
      "\n",
      "Epoch 427/500 --> loss = 0.11997999649236699, acc = 0.9597772283129172\n",
      "\n",
      "Epoch 428/500 --> loss = 0.11762088610984311, acc = 0.9604579198478472\n",
      "\n",
      "Epoch 429/500 --> loss = 0.11975110130439891, acc = 0.9593363262639187\n",
      "\n",
      "Epoch 430/500 --> loss = 0.11653012782335281, acc = 0.9600092815880729\n",
      "\n",
      "Epoch 431/500 --> loss = 0.11840334379732019, acc = 0.960047955560212\n",
      "\n",
      "Epoch 432/500 --> loss = 0.12023632988186166, acc = 0.9593440595239696\n",
      "\n",
      "Epoch 433/500 --> loss = 0.11790716485811932, acc = 0.9604115114353671\n",
      "\n",
      "Epoch 434/500 --> loss = 0.12003793545288614, acc = 0.9598468435872899\n",
      "\n",
      "Epoch 435/500 --> loss = 0.12187580277424047, acc = 0.9590191876534189\n",
      "\n",
      "Epoch 436/500 --> loss = 0.11885883179631564, acc = 0.9599087256016118\n",
      "\n",
      "Epoch 437/500 --> loss = 0.12080802281599233, acc = 0.9589418290865304\n",
      "\n",
      "Epoch 438/500 --> loss = 0.12159177830608764, acc = 0.9591352101599816\n",
      "\n",
      "Epoch 439/500 --> loss = 0.1218006242767419, acc = 0.9588335410203084\n",
      "\n",
      "Epoch 440/500 --> loss = 0.12021500263178703, acc = 0.9592589741886253\n",
      "\n",
      "Epoch 441/500 --> loss = 0.11908301578299833, acc = 0.960202662071379\n",
      "\n",
      "Epoch 442/500 --> loss = 0.11546236239742524, acc = 0.9617961019572645\n",
      "\n",
      "Epoch 443/500 --> loss = 0.1209067827699208, acc = 0.9588954218543402\n",
      "\n",
      "Epoch 444/500 --> loss = 0.11789527535438538, acc = 0.9599628708150127\n",
      "\n",
      "Epoch 445/500 --> loss = 0.12078567778710092, acc = 0.9597076124483996\n",
      "\n",
      "Epoch 446/500 --> loss = 0.12034475419780996, acc = 0.9587948640974442\n",
      "\n",
      "Epoch 447/500 --> loss = 0.11904727549541115, acc = 0.9601098375745339\n",
      "\n",
      "Epoch 448/500 --> loss = 0.12031957204684172, acc = 0.9591970921743034\n",
      "\n",
      "Epoch 449/500 --> loss = 0.12029353138243798, acc = 0.9591120074291041\n",
      "\n",
      "Epoch 450/500 --> loss = 0.11730118590121222, acc = 0.9605816809257658\n",
      "\n",
      "Epoch 451/500 --> loss = 0.11618719537659447, acc = 0.9610689996492745\n",
      "\n",
      "Epoch 452/500 --> loss = 0.1132032071599866, acc = 0.9619043935643564\n",
      "\n",
      "Epoch 453/500 --> loss = 0.11531873858801209, acc = 0.9611154098321896\n",
      "\n",
      "Epoch 454/500 --> loss = 0.11611538041051071, acc = 0.9612855828634583\n",
      "\n",
      "Epoch 455/500 --> loss = 0.11845237739605478, acc = 0.9598855193298642\n",
      "\n",
      "Epoch 456/500 --> loss = 0.11647456648326156, acc = 0.961092204150587\n",
      "\n",
      "Epoch 457/500 --> loss = 0.12139101075653982, acc = 0.9590655942954639\n",
      "\n",
      "Epoch 458/500 --> loss = 0.1185679362699537, acc = 0.9603032174676952\n",
      "\n",
      "Epoch 459/500 --> loss = 0.117752274221713, acc = 0.9598313747066083\n",
      "\n",
      "Epoch 460/500 --> loss = 0.11733462012345248, acc = 0.9604733922693989\n",
      "\n",
      "Epoch 461/500 --> loss = 0.12130324926116678, acc = 0.9589650388991479\n",
      "\n",
      "Epoch 462/500 --> loss = 0.11885553554143055, acc = 0.9601717207691457\n",
      "\n",
      "Epoch 463/500 --> loss = 0.1181886621040873, acc = 0.9599783391055494\n",
      "\n",
      "Epoch 464/500 --> loss = 0.11933554649943172, acc = 0.959792697783744\n",
      "\n",
      "Epoch 465/500 --> loss = 0.11704088872907185, acc = 0.9608369440135389\n",
      "\n",
      "Epoch 466/500 --> loss = 0.11713247701968296, acc = 0.9606977093337786\n",
      "\n",
      "Epoch 467/500 --> loss = 0.11800267333441442, acc = 0.9593672628449922\n",
      "\n",
      "Epoch 468/500 --> loss = 0.11724146735845226, acc = 0.9602490716641492\n",
      "\n",
      "Epoch 469/500 --> loss = 0.11915724990096423, acc = 0.9595993196610177\n",
      "\n",
      "Epoch 470/500 --> loss = 0.12058917204342266, acc = 0.9591042706281832\n",
      "\n",
      "Epoch 471/500 --> loss = 0.11905408783419298, acc = 0.9596534660547087\n",
      "\n",
      "Epoch 472/500 --> loss = 0.11799944317576909, acc = 0.9598623165989867\n",
      "\n",
      "Epoch 473/500 --> loss = 0.11974255408685987, acc = 0.959692139436703\n",
      "\n",
      "Epoch 474/500 --> loss = 0.11999075365538645, acc = 0.959529703206355\n",
      "\n",
      "Epoch 475/500 --> loss = 0.11609951096891176, acc = 0.9607905332404788\n",
      "\n",
      "Epoch 476/500 --> loss = 0.11721344314294287, acc = 0.9605352731034307\n",
      "\n",
      "Epoch 477/500 --> loss = 0.11634412695570748, acc = 0.9603032162874052\n",
      "\n",
      "Epoch 478/500 --> loss = 0.11889027641846402, acc = 0.9598700480886025\n",
      "\n",
      "Epoch 479/500 --> loss = 0.11660596595542265, acc = 0.9603186881188119\n",
      "\n",
      "Epoch 480/500 --> loss = 0.11664633666819865, acc = 0.9605894236281367\n",
      "\n",
      "Epoch 481/500 --> loss = 0.11480235625611673, acc = 0.9609452326699058\n",
      "\n",
      "Epoch 482/500 --> loss = 0.11949609942955546, acc = 0.959947399573751\n",
      "\n",
      "Epoch 483/500 --> loss = 0.1198126153633146, acc = 0.960055692361133\n",
      "\n",
      "Epoch 484/500 --> loss = 0.11678068819317487, acc = 0.9607673269687312\n",
      "\n",
      "Epoch 485/500 --> loss = 0.11423131535844047, acc = 0.961463490335068\n",
      "\n",
      "Epoch 486/500 --> loss = 0.11511259629289702, acc = 0.960937498229565\n",
      "\n",
      "Epoch 487/500 --> loss = 0.11727126739402809, acc = 0.9606435629400877\n",
      "\n",
      "Epoch 488/500 --> loss = 0.12017889844604057, acc = 0.9592589724181902\n",
      "\n",
      "Epoch 489/500 --> loss = 0.11815992757530495, acc = 0.9599396692644252\n",
      "\n",
      "Epoch 490/500 --> loss = 0.11793667874713935, acc = 0.9600170183889937\n",
      "\n",
      "Epoch 491/500 --> loss = 0.11754511481169427, acc = 0.9603573609106612\n",
      "\n",
      "Epoch 492/500 --> loss = 0.12011855361190173, acc = 0.9592435035375085\n",
      "\n",
      "Epoch 493/500 --> loss = 0.11614789253119195, acc = 0.9608446796341698\n",
      "\n",
      "Epoch 494/500 --> loss = 0.11602347034333955, acc = 0.96062036020921\n",
      "\n",
      "Epoch 495/500 --> loss = 0.11428295909473211, acc = 0.9612314382402023\n",
      "\n",
      "Epoch 496/500 --> loss = 0.1159501720478039, acc = 0.9612082290177298\n",
      "\n",
      "Epoch 497/500 --> loss = 0.11503672201444606, acc = 0.9611154074716096\n",
      "\n",
      "Epoch 498/500 --> loss = 0.11495784183244893, acc = 0.9609684448431034\n",
      "\n",
      "Epoch 499/500 --> loss = 0.11563344963706366, acc = 0.960519803042459\n",
      "\n",
      "Epoch 500/500 --> loss = 0.11691846071493507, acc = 0.9610303215461202\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 500\n",
    "total_loss = list()\n",
    "total_acc = list()\n",
    "for eps in range(EPOCHS):    \n",
    "    # print('\\nEpoch {}/{}'.format(eps + 1, EPOCHS))\n",
    "    losses, accs = [], []\n",
    "    for n_batches in range(N_BATCHES):\n",
    "        loss,acc = model.train_on_batch(X_values[n_batches],Y_values[n_batches])\n",
    "        losses.append(loss)\n",
    "        accs.append(acc)\n",
    "    print('\\nEpoch {}/{} --> loss = {}, acc = {}'.format(eps + 1, EPOCHS, np.average(losses), np.average(accs)))\n",
    "    total_loss.append(np.average(losses))\n",
    "    total_acc.append(np.average(accs))\n",
    "\n",
    "model.save('saved_models/model500.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = total_loss\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "plt.plot(epochs, total_acc, 'y', label='Training accuracy')\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (1, 1, 512)               44032     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (1, 1, 256)               787456    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (1, 1, 256)               0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (1, 1, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (1, 1, 256)               0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (1, 256)                  525312    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (1, 256)                  0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (1, 86)                   22102     \n",
      "=================================================================\n",
      "Total params: 1,904,214\n",
      "Trainable params: 1,904,214\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sample_model=Sequential()\n",
    "sample_model.add(Embedding(Vocab_length,512,batch_input_shape=(1,1)))\n",
    "sample_model.add(LSTM(256,return_sequences=True,stateful=True))\n",
    "sample_model.add(Dropout(0.3))\n",
    "sample_model.add(LSTM(256,return_sequences=True,stateful=True))\n",
    "sample_model.add(Dropout(0.2))\n",
    "sample_model.add(LSTM(256,stateful=True))\n",
    "sample_model.add(Dropout(0.2))\n",
    "sample_model.add(Dense(Vocab_length, activation='softmax'))\n",
    "\n",
    "sample_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Model: \"sequential_4\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "embedding_4 (Embedding)      (20, 64, 512)             44032     \n",
    "_________________________________________________________________\n",
    "lstm_12 (LSTM)               (20, 64, 256)             787456    \n",
    "_________________________________________________________________\n",
    "dropout_12 (Dropout)         (20, 64, 256)             0         \n",
    "_________________________________________________________________\n",
    "lstm_13 (LSTM)               (20, 64, 256)             525312    \n",
    "_________________________________________________________________\n",
    "dropout_13 (Dropout)         (20, 64, 256)             0         \n",
    "_________________________________________________________________\n",
    "lstm_14 (LSTM)               (20, 64, 256)             525312    \n",
    "_________________________________________________________________\n",
    "dropout_14 (Dropout)         (20, 64, 256)             0         \n",
    "_________________________________________________________________\n",
    "time_distributed_1 (TimeDist (20, 64, 86)              22102     \n",
    "=================================================================\n",
    "Total params: 1,904,214\n",
    "Trainable params: 1,904,214\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:40<00:00, 24.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M:6/8\n",
      "K:D\n",
      "A|\"D\"FED FED|FGA d2A|\"G\"Bcd \"D\"AGF|\"Em\"GEE \"A7\"E2A|\n",
      "\"D\"FED FED|\"D\"FGA A2A|\"G\"Bcd \"D\"Adf|\"A7\"edc \"D\"d2:|\n",
      "P:B\n",
      "A|\"D\"d2f a2f|\"G\"b2g \"A7\"a2f|\"D\"def agf|\"D\"f3 \"D7\"a3|\"G\"b2g \"A7\"efg|\\\n",
      "\"D\"a2f \"Bm\"d2A|\"D\"d2e f2d|\"D\"d3 fed|\"A\"dec A2e|\n",
      "\"D\"f2A ABA|\"Em\"G2G F2E|\"A7\"ABA GFE|\"D\"D3 -D2||\n",
      "\n",
      "\n",
      "X: 176\n",
      "T:Liberty Bell\n",
      "% Nottingham Music Database\n",
      "S:FTB, via EF\n",
      "Y:AB\n",
      "M:6/8\n",
      "K:A\n",
      "P:A\n",
      "|:E|\"A\"E2E EAB|\"A\"c3 A3|\"A\"c3 A2c|\"Bm\"B2c \"E\"d2B|\"A\"c3 E2A|\"A\"c2e ABc|\"Bm\"daf \"E7\"edB|\n",
      "\"A\"Ace a2g|\"D\"agf \"A\"edc|\"E7\"edB \"A\"A2:|\n",
      "P:B\n",
      "B|\"A\"c2e efe|\"A\"ecA Ace|\"D\"fgf fgf|\"A\"ecA A2c/2d/2|\n",
      "\"D\"agf \"F#m\"ecc|\"Bm\"B2c \"E7\"B2A|\"A\"Ace aae|\"Bm\"b2g \"E7\"fed|\"A\"c3 -c2E|\"D\"d2e f2e|\n",
      "\"A\"a2e c2A|\"A\"Ace a3|\"D\"agf \"A\"edc|\"E7\"edB \"A\"A2:|\n",
      "\n",
      "\n",
      "X: 303\n",
      "T:The Stone Court\n",
      "% Nottingham Music Database\n",
      "S:Chris Dewhurst 1989, via Phil Rowe\n",
      "M:6/8\n",
      "K:Am\n",
      "\"Am\"A3 a3|\"Em\"gee e2d|\"C\"c2g edc|\"G\"Bgd -dBG|\"Am\"A3 \"F\"a3|\"C\"gee e2d|\\\n",
      "\"Dm\"c2e \"E7\"dcB| [1\"Am\"A3 -AcB:|\n",
      " [2\"Am\" A3 -\"G7\"A2|||:B|\"C\"c3 -cef|\"C\"g3 e2d|\"C\"c2d e2d|\"C\"e3 z3|\"G\"d^cd B3|\n",
      "\"A7\"^c3 A3|\"Dm\"d2d \"G\"def|\"G\"g2f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def sample_model_maker():\n",
    "    sample_model=Sequential()\n",
    "    sample_model.add(Embedding(Vocab_length,512,batch_input_shape=(1,1)))\n",
    "    sample_model.add(LSTM(256,return_sequences=True,stateful=True))\n",
    "    sample_model.add(Dropout(0.3))\n",
    "    sample_model.add(LSTM(256,return_sequences=True,stateful=True))\n",
    "    sample_model.add(Dropout(0.2))\n",
    "    sample_model.add(LSTM(256,stateful=True))\n",
    "    sample_model.add(Dropout(0.2))\n",
    "    sample_model.add(Dense(Vocab_length, activation='softmax'))\n",
    "    return sample_model\n",
    "\n",
    "def sample_maker(n):\n",
    "    sam_model = sample_model_maker()\n",
    "    sam_model.load_weights('saved_models/model500.h5')\n",
    "\n",
    "    output = list()\n",
    "    for i in tqdm(range(n)):\n",
    "        inp = np.zeros((1, 1))\n",
    "        if output:\n",
    "            inp[0, 0] = output[-1]\n",
    "        else:\n",
    "            inp[0, 0] = np.random.randint(Vocab_length)\n",
    "        result = sam_model.predict(inp)[0]\n",
    "        sample = np.random.choice(range(Vocab_length), p=result)\n",
    "        output.append(sample)  \n",
    "    print(''.join([int_to_char[value] for value in output]))\n",
    "    \n",
    "sample_maker(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_model=Sequential()\n",
    "# sample_model.add(Embedding(Vocab_length,512,batch_input_shape=(1,1)))\n",
    "# sample_model.add(LSTM(256,return_sequences=True,stateful=True))\n",
    "# sample_model.add(Dropout(0.3))\n",
    "# sample_model.add(LSTM(256,return_sequences=True,stateful=True))\n",
    "# sample_model.add(Dropout(0.2))\n",
    "# sample_model.add(LSTM(256,stateful=True))\n",
    "# sample_model.add(Dropout(0.2))\n",
    "# sample_model.add(Dense(Vocab_length, activation='softmax'))\n",
    "\n",
    "# sample_model.load_weights('saved_models/model300.h5')\n",
    "\n",
    "\n",
    "# output = [np.reshape(np.random.randint(Vocab_length),(1,1))]\n",
    "# for i in tqdm(range(1000)):\n",
    "#     inp = output[-1]\n",
    "#     result = sample_model.predict(inp)[0]\n",
    "#     sample = np.random.choice(range(Vocab_length), p=result)\n",
    "#     sample = sample.reshape(1,1)\n",
    "#     output.append(sample)  \n",
    "# print(''.join([int_to_char[value[0][0]] for value in output]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sir's Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_batches(T, vocab_size):\n",
    "    length = T.shape[0]; #129,665\n",
    "    batch_chars = int(length / BATCH_SIZE); # 8,104\n",
    "\n",
    "    for start in range(0, batch_chars - SEQ_LENGTH, SEQ_LENGTH): # (0, 8040, 64)\n",
    "        X = np.zeros((BATCH_SIZE, SEQ_LENGTH)) # 16X64\n",
    "        Y = np.zeros((BATCH_SIZE, SEQ_LENGTH, vocab_size)) # 16X64X86\n",
    "        for batch_idx in range(0, BATCH_SIZE): # (0,16)\n",
    "            for i in range(0, SEQ_LENGTH): #(0,64)\n",
    "                X[batch_idx, i] = T[batch_chars * batch_idx + start + i] # \n",
    "                Y[batch_idx, i, T[batch_chars * batch_idx + start + i + 1]] = 1\n",
    "        yield X, Y\n",
    "        \n",
    "def build_model(batch_size, seq_len, vocab_size):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 512, batch_input_shape=(batch_size, seq_len)))\n",
    "    for i in range(3):\n",
    "        model.add(LSTM(256, return_sequences=True, stateful=True))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(TimeDistributed(Dense(vocab_size))) \n",
    "    model.add(Activation('softmax'))\n",
    "    return model\n",
    "\n",
    "def build_sample_model(vocab_size):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 512, batch_input_shape=(1, 1)))\n",
    "    for i in range(3):\n",
    "        model.add(LSTM(256, return_sequences=(i != 2), stateful=True))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(vocab_size))\n",
    "    model.add(Activation('softmax'))\n",
    "    return model\n",
    "\n",
    "def give_sample(num_chars,idx_to_char,vocab_size):\n",
    "    model = build_sample_model(vocab_size)\n",
    "    model.load_weights('saved_models/sirmodel.h5')\n",
    "\n",
    "    sampled = list()\n",
    "\n",
    "    for i in range(num_chars):\n",
    "        batch = np.zeros((1, 1))\n",
    "        if sampled:\n",
    "            batch[0, 0] = sampled[-1]\n",
    "        else:\n",
    "            batch[0, 0] = np.random.randint(vocab_size)\n",
    "        result = model.predict_on_batch(batch).ravel()\n",
    "        sample = np.random.choice(range(vocab_size), p=result)\n",
    "        sampled.append(sample)\n",
    "    return sampled\n",
    "    # return ''.join(idx_to_char[c] for c in sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (16, 64, 512)             44032     \n",
      "_________________________________________________________________\n",
      "lstm_36 (LSTM)               (16, 64, 256)             787456    \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (16, 64, 256)             0         \n",
      "_________________________________________________________________\n",
      "lstm_37 (LSTM)               (16, 64, 256)             525312    \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (16, 64, 256)             0         \n",
      "_________________________________________________________________\n",
      "lstm_38 (LSTM)               (16, 64, 256)             525312    \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (16, 64, 256)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_8 (TimeDist (16, 64, 86)              22102     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (16, 64, 86)              0         \n",
      "=================================================================\n",
      "Total params: 1,904,214\n",
      "Trainable params: 1,904,214\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Epoch 1/100: loss = 3.4649293422698975, acc = 0.130859375\n",
      "\n",
      "Epoch 2/100: loss = 3.192575454711914, acc = 0.1640625\n",
      "\n",
      "Epoch 3/100: loss = 2.3815248012542725, acc = 0.33984375\n",
      "\n",
      "Epoch 4/100: loss = 1.8613660335540771, acc = 0.4794921875\n",
      "\n",
      "Epoch 5/100: loss = 1.6368862390518188, acc = 0.51171875\n",
      "\n",
      "Epoch 6/100: loss = 1.5598552227020264, acc = 0.5458984375\n",
      "\n",
      "Epoch 7/100: loss = 1.5177135467529297, acc = 0.560546875\n",
      "\n",
      "Epoch 8/100: loss = 1.4953339099884033, acc = 0.5576171875\n",
      "\n",
      "Epoch 9/100: loss = 1.4291008710861206, acc = 0.564453125\n",
      "\n",
      "Epoch 10/100: loss = 1.3801655769348145, acc = 0.5927734375\n",
      "\n",
      "Epoch 11/100: loss = 1.3446273803710938, acc = 0.5986328125\n",
      "\n",
      "Epoch 12/100: loss = 1.2837157249450684, acc = 0.611328125\n",
      "\n",
      "Epoch 13/100: loss = 1.2533912658691406, acc = 0.6259765625\n",
      "\n",
      "Epoch 14/100: loss = 1.2306361198425293, acc = 0.6337890625\n",
      "\n",
      "Epoch 15/100: loss = 1.177581787109375, acc = 0.650390625\n",
      "\n",
      "Epoch 16/100: loss = 1.1442869901657104, acc = 0.640625\n",
      "\n",
      "Epoch 17/100: loss = 1.113701581954956, acc = 0.658203125\n",
      "\n",
      "Epoch 18/100: loss = 1.0936896800994873, acc = 0.6572265625\n",
      "\n",
      "Epoch 19/100: loss = 1.071665644645691, acc = 0.6689453125\n",
      "\n",
      "Epoch 20/100: loss = 1.000293493270874, acc = 0.6923828125\n",
      "\n",
      "Epoch 21/100: loss = 0.9775555729866028, acc = 0.69140625\n",
      "\n",
      "Epoch 22/100: loss = 0.9778896570205688, acc = 0.6943359375\n",
      "\n",
      "Epoch 23/100: loss = 0.9496698975563049, acc = 0.7080078125\n",
      "\n",
      "Epoch 24/100: loss = 0.9451232552528381, acc = 0.7109375\n",
      "\n",
      "Epoch 25/100: loss = 0.9149783253669739, acc = 0.7177734375\n",
      "\n",
      "Epoch 26/100: loss = 0.9104422330856323, acc = 0.720703125\n",
      "\n",
      "Epoch 27/100: loss = 0.8862330913543701, acc = 0.7353515625\n",
      "\n",
      "Epoch 28/100: loss = 0.846766471862793, acc = 0.7392578125\n",
      "\n",
      "Epoch 29/100: loss = 0.8145520091056824, acc = 0.7490234375\n",
      "\n",
      "Epoch 30/100: loss = 0.8066686391830444, acc = 0.7451171875\n",
      "\n",
      "Epoch 31/100: loss = 0.8001089096069336, acc = 0.744140625\n",
      "\n",
      "Epoch 32/100: loss = 0.8085073232650757, acc = 0.75390625\n",
      "\n",
      "Epoch 33/100: loss = 0.7639089226722717, acc = 0.7705078125\n",
      "\n",
      "Epoch 34/100: loss = 0.747643768787384, acc = 0.76953125\n",
      "\n",
      "Epoch 35/100: loss = 0.7106410264968872, acc = 0.7822265625\n",
      "\n",
      "Epoch 36/100: loss = 0.6812601089477539, acc = 0.7783203125\n",
      "\n",
      "Epoch 37/100: loss = 0.7046301960945129, acc = 0.7724609375\n",
      "\n",
      "Epoch 38/100: loss = 0.7004178762435913, acc = 0.779296875\n",
      "\n",
      "Epoch 39/100: loss = 0.6884276866912842, acc = 0.7744140625\n",
      "\n",
      "Epoch 40/100: loss = 0.622596025466919, acc = 0.8046875\n",
      "\n",
      "Epoch 41/100: loss = 0.6216466426849365, acc = 0.7978515625\n",
      "\n",
      "Epoch 42/100: loss = 0.6251756548881531, acc = 0.80078125\n",
      "\n",
      "Epoch 43/100: loss = 0.5981656312942505, acc = 0.8056640625\n",
      "\n",
      "Epoch 44/100: loss = 0.6330854892730713, acc = 0.80859375\n",
      "\n",
      "Epoch 45/100: loss = 0.5731856822967529, acc = 0.8076171875\n",
      "\n",
      "Epoch 46/100: loss = 0.5505937337875366, acc = 0.8291015625\n",
      "\n",
      "Epoch 47/100: loss = 0.5662135481834412, acc = 0.8173828125\n",
      "\n",
      "Epoch 48/100: loss = 0.557709813117981, acc = 0.8212890625\n",
      "\n",
      "Epoch 49/100: loss = 0.5354163646697998, acc = 0.8251953125\n",
      "\n",
      "Epoch 50/100: loss = 0.5387965440750122, acc = 0.826171875\n",
      "\n",
      "Epoch 51/100: loss = 0.5220348238945007, acc = 0.8349609375\n",
      "\n",
      "Epoch 52/100: loss = 0.5375255346298218, acc = 0.826171875\n",
      "\n",
      "Epoch 53/100: loss = 0.4707913100719452, acc = 0.8466796875\n",
      "\n",
      "Epoch 54/100: loss = 0.5211749076843262, acc = 0.8330078125\n",
      "\n",
      "Epoch 55/100: loss = 0.4856227934360504, acc = 0.8564453125\n",
      "\n",
      "Epoch 56/100: loss = 0.46370911598205566, acc = 0.8427734375\n",
      "\n",
      "Epoch 57/100: loss = 0.4816044270992279, acc = 0.8466796875\n",
      "\n",
      "Epoch 58/100: loss = 0.45235884189605713, acc = 0.849609375\n",
      "\n",
      "Epoch 59/100: loss = 0.45091843605041504, acc = 0.8505859375\n",
      "\n",
      "Epoch 60/100: loss = 0.46576040983200073, acc = 0.857421875\n",
      "\n",
      "Epoch 61/100: loss = 0.46606865525245667, acc = 0.8466796875\n",
      "\n",
      "Epoch 62/100: loss = 0.42498156428337097, acc = 0.8603515625\n",
      "\n",
      "Epoch 63/100: loss = 0.40931230783462524, acc = 0.8671875\n",
      "\n",
      "Epoch 64/100: loss = 0.4259101450443268, acc = 0.876953125\n",
      "\n",
      "Epoch 65/100: loss = 0.39408907294273376, acc = 0.8681640625\n",
      "\n",
      "Epoch 66/100: loss = 0.4017792046070099, acc = 0.87890625\n",
      "\n",
      "Epoch 67/100: loss = 0.3851158916950226, acc = 0.8798828125\n",
      "\n",
      "Epoch 68/100: loss = 0.405830442905426, acc = 0.869140625\n",
      "\n",
      "Epoch 69/100: loss = 0.40268293023109436, acc = 0.8662109375\n",
      "\n",
      "Epoch 70/100: loss = 0.3898766338825226, acc = 0.876953125\n",
      "\n",
      "Epoch 71/100: loss = 0.37912121415138245, acc = 0.8779296875\n",
      "\n",
      "Epoch 72/100: loss = 0.36448734998703003, acc = 0.8798828125\n",
      "\n",
      "Epoch 73/100: loss = 0.3515077829360962, acc = 0.8798828125\n",
      "\n",
      "Epoch 74/100: loss = 0.3440377116203308, acc = 0.8974609375\n",
      "\n",
      "Epoch 75/100: loss = 0.3445594310760498, acc = 0.888671875\n",
      "\n",
      "Epoch 76/100: loss = 0.3631571829319, acc = 0.890625\n",
      "\n",
      "Epoch 77/100: loss = 0.34220391511917114, acc = 0.8974609375\n",
      "\n",
      "Epoch 78/100: loss = 0.3548527657985687, acc = 0.8828125\n",
      "\n",
      "Epoch 79/100: loss = 0.319053590297699, acc = 0.9033203125\n",
      "\n",
      "Epoch 80/100: loss = 0.326045423746109, acc = 0.892578125\n",
      "\n",
      "Epoch 81/100: loss = 0.3504451513290405, acc = 0.8896484375\n",
      "\n",
      "Epoch 82/100: loss = 0.3079026937484741, acc = 0.9130859375\n",
      "\n",
      "Epoch 83/100: loss = 0.32196080684661865, acc = 0.9013671875\n",
      "\n",
      "Epoch 84/100: loss = 0.35966673493385315, acc = 0.888671875\n",
      "\n",
      "Epoch 85/100: loss = 0.3376327157020569, acc = 0.8984375\n",
      "\n",
      "Epoch 86/100: loss = 0.3033348321914673, acc = 0.89453125\n",
      "\n",
      "Epoch 87/100: loss = 0.3144000172615051, acc = 0.8984375\n",
      "\n",
      "Epoch 88/100: loss = 0.2948729693889618, acc = 0.8994140625\n",
      "\n",
      "Epoch 89/100: loss = 0.33562618494033813, acc = 0.8955078125\n",
      "\n",
      "Epoch 90/100: loss = 0.30996114015579224, acc = 0.90234375\n",
      "\n",
      "Epoch 91/100: loss = 0.32040131092071533, acc = 0.888671875\n",
      "\n",
      "Epoch 92/100: loss = 0.29635563492774963, acc = 0.89453125\n",
      "\n",
      "Epoch 93/100: loss = 0.3044966757297516, acc = 0.90625\n",
      "\n",
      "Epoch 94/100: loss = 0.2629786729812622, acc = 0.921875\n",
      "\n",
      "Epoch 95/100: loss = 0.2642056345939636, acc = 0.9150390625\n",
      "\n",
      "Epoch 96/100: loss = 0.2795824706554413, acc = 0.9189453125\n",
      "\n",
      "Epoch 97/100: loss = 0.25021350383758545, acc = 0.921875\n",
      "\n",
      "Epoch 98/100: loss = 0.2825712561607361, acc = 0.9033203125\n",
      "\n",
      "Epoch 99/100: loss = 0.25426575541496277, acc = 0.9130859375\n",
      "\n",
      "Epoch 100/100: loss = 0.28993529081344604, acc = 0.912109375\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 16\n",
    "SEQ_LENGTH = 64\n",
    "\n",
    "text = open('data/input.txt','r').read()\n",
    "char_to_idx = { ch: i for (i, ch) in enumerate(sorted(list(set(text)))) }\n",
    "idx_to_char = { i: ch for (ch, i) in char_to_idx.items() }\n",
    "vocab_size = len(char_to_idx)\n",
    "T = np.asarray([char_to_idx[c] for c in text], dtype=np.int32)\n",
    "\n",
    "model = build_model(BATCH_SIZE, SEQ_LENGTH, vocab_size)\n",
    "model.summary();\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "E = 100\n",
    "for epoch in range(E):\n",
    "    # print('\\nEpoch {}/{}'.format(epoch + 1, E))\n",
    "    \n",
    "    losses, accs = [], []\n",
    "         \n",
    "    for i, (X, Y) in enumerate(read_batches(T, vocab_size)):\n",
    "        loss, acc = model.train_on_batch(X, Y)\n",
    "        # print('Batch {}: loss = {}, acc = {}'.format(i + 1, loss, acc))\n",
    "        losses.append(loss)\n",
    "        accs.append(acc)\n",
    "    print('\\nEpoch {}/{}: loss = {}, acc = {}'.format(epoch + 1, E, loss, acc))\n",
    "model.save('saved_models/sirmodel.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TOdo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have trained this model using train_on_batch function\n",
    "\n",
    "\n",
    "so now we need to try by direct .fit() method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ae9f9591411ffd0bfff2881679534053d07baeb3b196689df39d16216e109346"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('tfod3': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
